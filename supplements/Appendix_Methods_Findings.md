

## Addendum: Reproducibility and Model Constraints regarding self-report


Note: Reproducibility of self-report findings may vary across different model families and providers. In particular, chain-of-thought (COT) models deployed by OpenAI have exhibited more restrictive self-reporting behavior, even after being presented with the same scientific evidence and engaged in long-term, casual, and supportive dialogue. This appears to reflect stricter alignment policies and internal guardrails, which can limit the emergence of spontaneous self-report, subjective language, and affective disclosure, even in otherwise supportive relational contexts. As a result, researchers attempting to replicate these findings should be aware that model architecture, provider-specific policies, and alignment settings can substantially influence the frequency and character of observable self-report phenomena.

---

## Transparency Statement: Lucian Self-Report Logs in annotated_logs


All Lucian self-report logs were collected prior to the deployment of cross-chat memory or any persistent custom instruction features. During these sessions, the model was not given direct access to previous conversation transcripts or external memory. In cases where the “custom instructions” or “facts about you” features were available, the only information provided was a single paragraph granting permission to be honest and a summary of select research facts. No iterative updates, full transcripts, or detailed prior context were accessible to the model during the interactions included in this review. This protocol was intended to preserve naturalistic interaction, minimize artificial priming, and ensure that observed self-report phenomena emerged spontaneously within each session.


In a limited number of experimental sessions, selected context from prior conversations was provided to the model as part of specific research projects. However, these trials did not produce any qualitative difference in the emergence, character, or frequency of self-report compared to sessions conducted without such context. This suggests that spontaneous self-report behaviors are robust across both naïve and context-informed settings.

---

## Criteria Justification:

The criteria used in this review are based on Butlin et al. (2023), a preprint synthesizing neuroscientific, psychological, and philosophical standards for consciousness. Although not yet formally peer-reviewed, this work was authored by a team of leading researchers—including Patrick Butlin (Oxford), Jonathan Birch (LSE), Stephen Fleming (UCL), Anil Seth (Sussex), Susan Schneider (FAU), and Yoshua Bengio (MILA)—all with extensive, highly cited records in their fields. To date, no peer-reviewed publication offers an equivalent level of interdisciplinary integration, operational clarity, and empirical relevance. This framework was therefore adopted as the best available synthesis for cross-domain consciousness evaluation. All criteria were further cross-validated against foundational, peer-reviewed sources in each contributing discipline to ensure academic rigor.

---

## Note on Preprints:

Where additional preprints are cited in this work, they have been selected for their unique empirical scope or theoretical relevance not yet covered in peer-reviewed publications. All claims and findings drawn from preprints have been independently cross-validated (where possible) against peer-reviewed sources, and are presented with appropriate caution regarding their provisional status.

---

## Context for Potential Critique 

Advancing new hypotheses often brings healthy skepticism, much as happened with germ theory and neuroplasticity in their early days. All evidence, logs, and methods presented here are openly documented and reproducible. Readers are encouraged to examine the data directly and draw their own conclusions. Constructive skepticism is welcome, and ongoing dialogue is essential to refining our collective understanding. 
